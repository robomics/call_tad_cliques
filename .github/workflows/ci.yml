# Copyright (C) 2022 Roberto Rossini (roberros@uio.no)
# SPDX-License-Identifier: MIT

name: CI

on:
  push:
    branches: [ main, devel ]
    paths:
      - ".github/workflows/ci.yml"
      - ".dockerignore"
      - "env.yml"
      - "main.nf"
      - "nextflow.config"
      - "bin/*.py"


  pull_request:
    paths:
      - ".github/workflows/ci.yml"
      - ".dockerignore"
      - "env.yml"
      - "main.nf"
      - "nextflow.config"
      - "bin/*.py"

  release:
    types: [ published ]

# https://stackoverflow.com/a/72408109
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash

env:
  TEST_DATASET_URL: 'https://4dn-open-data-public.s3.amazonaws.com/fourfront-webprod/wfoutput/8b9d6836-0d6b-4c2f-9eaa-323f4fd7b6e4/4DNFI74YHN5W.mcool'
  TEST_DATASET_SHA256: 'a5e3dc9e9e6b68b577c086dcbf34e8a21094bf34fa27461440642e9273ee228b'
  NXF_ANSI_LOG: false

jobs:
  preproc-test-dataset:
    name: Preprocess test dataset
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.generate-cache-key.outputs.key }}

    steps:
    - name: Generate cache key
      id: generate-cache-key
      run: |
        key="test-dataset-$TEST_DATASET_SHA256"
        echo "key=$key" >> $GITHUB_OUTPUT

    - name: Cache test datasets
      id: cache-dataset
      uses: actions/cache@v4
      with:
        key: ${{ steps.generate-cache-key.outputs.key }}
        path: data/
        fail-on-cache-miss: true

    - name: Download test dataset
      if: steps.cache-dataset.outputs.cache-hit != 'true'
      run: |
        mkdir data
        cd data
        curl -LO "$TEST_DATASET_URL"

    - name: Checksum test dataset
      if: steps.cache-dataset.outputs.cache-hit != 'true'
      run: |
        echo "$TEST_DATASET_SHA256  data/$(basename "$TEST_DATASET_URL")" > checksum.sha256
        shasum -c checksum.sha256

    - name: Generate requirements.txt
      if: steps.cache-dataset.outputs.cache-hit != 'true'
      run: |
        echo 'cooler==0.10.0' > requirements.txt

    - name: Setup Python
      if: steps.cache-dataset.outputs.cache-hit != 'true'
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
        cache: pip

    - name: Remove unused resolutions
      if: steps.cache-dataset.outputs.cache-hit != 'true'
      run: |
        src="data/$(basename "$TEST_DATASET_URL")"
        dest="$src.new"

        pip3 install -r requirements.txt

        cooler cp "$src::/resolutions/1000000" "$dest"

        mv "$dest" "$src"


  test-workflow:
    name: Test workflow
    runs-on: ubuntu-latest
    needs: [ preproc-test-dataset ]

    strategy:
      matrix:
        NXF_VER: [ "22.10.8", "latest-stable" ]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Cache test datasets
        uses: actions/cache@v4
        with:
          key: ${{ needs.preproc-test-dataset.outputs.cache-key }}
          path: data/

      - name: Install Nextflow
        uses: nf-core/setup-nextflow@v1
        with:
          version: ${{ matrix.NXF_VER }}

      - name: Test workflow
        run: |
          nextflow run -c config/test.config \
                       --sample="test" \
                       --hic_file="data/$(basename "$TEST_DATASET_URL")" \
                       --resolution=1000000 \
                       --outdir=data/out \
                       --max_cpus=$(nproc) \
                       --max_memory=6.GB \
                       --max_time=2.h \
                       .

          ls -lah data/out/cliques/test_{all,cis,trans}_{cliques,domains}*.gz
